{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/nicost312/two-branch-inception-v3?scriptVersionId=180543365\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport gc\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import color\nimport json\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# test\n# testing\n# testing2\n# from google.colab import files","metadata":{"id":"L5QrUWbbdpMs","execution":{"iopub.status.busy":"2024-05-30T09:17:57.686695Z","iopub.execute_input":"2024-05-30T09:17:57.687173Z","iopub.status.idle":"2024-05-30T09:17:57.693730Z","shell.execute_reply.started":"2024-05-30T09:17:57.687132Z","shell.execute_reply":"2024-05-30T09:17:57.692693Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(\"TensorFlow version: \", tf.__version__)\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:17:57.695875Z","iopub.execute_input":"2024-05-30T09:17:57.696480Z","iopub.status.idle":"2024-05-30T09:17:57.708159Z","shell.execute_reply.started":"2024-05-30T09:17:57.696444Z","shell.execute_reply":"2024-05-30T09:17:57.706719Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"TensorFlow version:  2.15.0\nGPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nif tf.test.gpu_device_name():\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n    print(\"Please install GPU version of TF\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:17:57.710105Z","iopub.execute_input":"2024-05-30T09:17:57.710874Z","iopub.status.idle":"2024-05-30T09:17:57.722862Z","shell.execute_reply.started":"2024-05-30T09:17:57.710814Z","shell.execute_reply":"2024-05-30T09:17:57.721699Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Default GPU Device: /device:GPU:0\n","output_type":"stream"}]},{"cell_type":"code","source":"def split_data(dataset, train_size, valid_size, test_size):\n  total_length = len(dataset)\n  train_length = int(np.floor(total_length * train_size))\n  valid_length = int(np.floor(total_length * valid_size))\n\n  train_data = dataset.take(train_length)\n  remaining = dataset.skip(train_length)\n  valid_data = remaining.take(valid_length)\n  test_data = remaining.skip(valid_length)\n\n#   train_x = x[:train_breakpoint]\n#   train_y = y[:train_breakpoint]\n#   valid_x = x[train_breakpoint:valid_breakpoint]\n#   valid_y = y[train_breakpoint:valid_breakpoint]\n#   test_x = x[valid_breakpoint:]\n#   test_y = y[valid_breakpoint:]\n\n  return train_data, valid_data, test_data","metadata":{"id":"3-OifnOBtjFP","execution":{"iopub.status.busy":"2024-05-30T09:17:57.726236Z","iopub.execute_input":"2024-05-30T09:17:57.726969Z","iopub.status.idle":"2024-05-30T09:17:57.734997Z","shell.execute_reply.started":"2024-05-30T09:17:57.726923Z","shell.execute_reply":"2024-05-30T09:17:57.733740Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def convert_to_lab(image):\n    image = np.array(image)\n    # OpenCV's 'cvtColor' function expects the input image to be in the BGR color space,\n    # so we need to convert from RGB to BGR first\n    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n    # Now we can convert from BGR to CIE L*a*b*\n    image_lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2Lab)\n\n    return image_lab","metadata":{"id":"UwAa7jMJyRWe","execution":{"iopub.status.busy":"2024-05-30T09:17:57.736615Z","iopub.execute_input":"2024-05-30T09:17:57.737516Z","iopub.status.idle":"2024-05-30T09:17:57.746300Z","shell.execute_reply.started":"2024-05-30T09:17:57.737477Z","shell.execute_reply":"2024-05-30T09:17:57.745197Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(img, label, use_lab_color_space=True):\n  img = tf.image.resize(img, (128, 128))\n  img /= 255.\n\n  if (use_lab_color_space):\n    img = tf.py_function(func=color.rgb2lab, inp=[img], Tout=tf.float32)\n    img = (img + [0, 128, 128]) / [100, 255, 255]\n    \n#     img = color.rgb2lab(img, channel_axis=2)\n#     img[:, :, 0] = img[:, :, 0] / 100\n#     img[:, :, 1:] = (img[:, :, 1:] + 128) / 255\n\n  img.set_shape((128, 128, 3))\n  label = tf.one_hot(label, depth=38)\n\n  return img, label","metadata":{"id":"iVmY-bBfu02N","execution":{"iopub.status.busy":"2024-05-30T09:17:57.748208Z","iopub.execute_input":"2024-05-30T09:17:57.748691Z","iopub.status.idle":"2024-05-30T09:17:57.757310Z","shell.execute_reply.started":"2024-05-30T09:17:57.748651Z","shell.execute_reply":"2024-05-30T09:17:57.756183Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Plot the validation and training curves separately\ndef plot_loss_curves(history):\n    \"\"\"\n    Returns separate loss curves for training and validation metrics\n    \"\"\"\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n\n    epochs = range(len(history.history['loss'])) # How many epochs\n\n\n    # Plot loss\n    plt.plot(epochs, loss, label=\"training_loss\")\n    plt.plot(epochs, val_loss, label=\"val_loss\")\n    plt.title(\"loss\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    # Plot accuracy\n    plt.figure()\n    plt.plot(epochs, accuracy, label=\"training_accuracy\")\n    plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n    plt.title(\"accuracy\")\n    plt.xlabel(\"epochs\")\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:17:57.759026Z","iopub.execute_input":"2024-05-30T09:17:57.760392Z","iopub.status.idle":"2024-05-30T09:17:57.770293Z","shell.execute_reply.started":"2024-05-30T09:17:57.760352Z","shell.execute_reply":"2024-05-30T09:17:57.769462Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# PLANT VILLAGE","metadata":{"id":"TMz-YGCRxsqb"}},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"def load_images(filepath):\n    images = []\n    labels = []\n    \n    folds = os.listdir(filepath)\n    for fold in folds:\n        foldpath = os.path.join(filepath, fold)\n        filelist = os.listdir(foldpath)\n        for file in filelist:\n            fpath = os.path.join(foldpath, file)\n            image = cv2.imread(fpath)\n            images.append(image)\n            labels.append(fold)\n        break\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:17:57.783360Z","iopub.execute_input":"2024-05-30T09:17:57.784152Z","iopub.status.idle":"2024-05-30T09:17:57.791619Z","shell.execute_reply.started":"2024-05-30T09:17:57.784115Z","shell.execute_reply":"2024-05-30T09:17:57.790666Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/plantvillage-dataset/color'\n\nimages, labels = load_images(path)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:17:57.796210Z","iopub.execute_input":"2024-05-30T09:17:57.797230Z","iopub.status.idle":"2024-05-30T09:18:01.638579Z","shell.execute_reply.started":"2024-05-30T09:17:57.797190Z","shell.execute_reply":"2024-05-30T09:18:01.637647Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(f'Number of images: {len(images)}')\nprint(f'Number of labels: {len(labels)}')\n\nprint(images[0].shape)\nprint(labels[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:01.639773Z","iopub.execute_input":"2024-05-30T09:18:01.640082Z","iopub.status.idle":"2024-05-30T09:18:01.646258Z","shell.execute_reply.started":"2024-05-30T09:18:01.640054Z","shell.execute_reply":"2024-05-30T09:18:01.645153Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Number of images: 1909\nNumber of labels: 1909\n(256, 256, 3)\nTomato___Late_blight\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_to_lab(images):\n    images = np.array(images)\n    images_lab = []\n    \n    for image in images:\n        # OpenCV's 'cvtColor' function expects the input image to be in the BGR color space,\n        # so we need to convert from RGB to BGR first\n        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n        # Now we can convert from BGR to CIE L*a*b*\n        image_lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2Lab)\n        images_lab.append(image_lab)\n\n    return images_lab","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:56.302666Z","iopub.execute_input":"2024-05-30T09:18:56.303381Z","iopub.status.idle":"2024-05-30T09:18:56.309155Z","shell.execute_reply.started":"2024-05-30T09:18:56.303345Z","shell.execute_reply":"2024-05-30T09:18:56.308173Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(images, labels):\n    # img\n    images = tf.image.resize(images, (128, 128))\n    images /= 255.\n\n    # lab\n    images = tf.py_function(func=convert_to_lab, inp=[images], Tout=tf.float32)\n    images = (images + [0, 128, 128]) / [100, 255, 255]\n    images.set_shape((128, 128, 3))\n    \n    #label\n    # Step 2: Use Tokenizer to encode the labels\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(labels)\n    numerical_labels = tokenizer.texts_to_sequences(labels)\n\n    # The Tokenizer outputs a list of lists, so we need to flatten it\n    numerical_labels = np.array(numerical_labels).flatten()\n\n    # Step 3: Apply one-hot encoding\n    num_classes = len(tokenizer.word_index)\n    labels = tf.keras.utils.to_categorical(numerical_labels, num_classes=num_classes + 1)\n    \n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:27:41.808863Z","iopub.execute_input":"2024-05-30T09:27:41.809342Z","iopub.status.idle":"2024-05-30T09:27:41.817025Z","shell.execute_reply.started":"2024-05-30T09:27:41.809303Z","shell.execute_reply":"2024-05-30T09:27:41.815964Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# get 50 first images\nimages = images[:50]\nlabels = labels[:50]\n\nimages_preprocessed, labels_preprocessed = preprocess_data(images, labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:28:50.792564Z","iopub.execute_input":"2024-05-30T09:28:50.792945Z","iopub.status.idle":"2024-05-30T09:28:57.160196Z","shell.execute_reply.started":"2024-05-30T09:28:50.792915Z","shell.execute_reply":"2024-05-30T09:28:57.159328Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"End input","metadata":{}},{"cell_type":"code","source":"# Construct a tf.data.Dataset\ndataset, ds_info = tfds.load('plant_village',\n              split=\"train\",\n              with_info=True,\n              shuffle_files=True,\n              as_supervised=True)","metadata":{"id":"HKbIPLaWoepQ","execution":{"iopub.status.busy":"2024-05-30T09:18:08.135271Z","iopub.status.idle":"2024-05-30T09:18:08.135606Z","shell.execute_reply.started":"2024-05-30T09:18:08.135427Z","shell.execute_reply":"2024-05-30T09:18:08.135440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shuffle(500)","metadata":{"id":"vZS07EWwU-6G","outputId":"013fb362-795a-4a78-ef0b-e8fab019776f","execution":{"iopub.status.busy":"2024-05-30T09:18:08.136557Z","iopub.status.idle":"2024-05-30T09:18:08.136885Z","shell.execute_reply.started":"2024-05-30T09:18:08.136732Z","shell.execute_reply":"2024-05-30T09:18:08.136745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(dataset))","metadata":{"id":"r7rYh7mhuKUf","outputId":"a379cf96-98d3-4360-f47b-1dc86c9f2604","execution":{"iopub.status.busy":"2024-05-30T09:18:08.138556Z","iopub.status.idle":"2024-05-30T09:18:08.138914Z","shell.execute_reply.started":"2024-05-30T09:18:08.138748Z","shell.execute_reply":"2024-05-30T09:18:08.138763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfds.show_examples(dataset, ds_info)","metadata":{"id":"PSNuUkE_rsY-","outputId":"fbf39e03-dbdc-4126-d364-04e3b810493b","execution":{"iopub.status.busy":"2024-05-30T09:18:08.140534Z","iopub.status.idle":"2024-05-30T09:18:08.140890Z","shell.execute_reply.started":"2024-05-30T09:18:08.140725Z","shell.execute_reply":"2024-05-30T09:18:08.140739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:08.142667Z","iopub.status.idle":"2024-05-30T09:18:08.143005Z","shell.execute_reply.started":"2024-05-30T09:18:08.142841Z","shell.execute_reply":"2024-05-30T09:18:08.142855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iterator = iter(dataset)\ntest= next(iterator)\n\n# Search for data with early blight condition\nif (np.argmax(test[1]) != 19):\n    test = next(iterator)\n    \ntest = np.copy(test[0])\n\nl_channel = test[:, :, 0]\n\nab_channel = test[:, :, 1:]\nones_array = np.zeros((ab_channel.shape[0], ab_channel.shape[1], 1))\nab_channel = np.concatenate((ones_array, ab_channel), axis=2)\nab_channel = (ab_channel * [100, 255, 255] - [0, 128, 128]) * [0, 255, 255]\nprint(ab_channel[0][0])\nab_channel = color.lab2rgb(ab_channel)\n\nprint(ab_channel[0][0])\n\n# Plot the image\nfig, axs = plt.subplots(1, 2, figsize=(10, 10))\nfig.subplots_adjust(hspace=0.5)\naxs[0].imshow(l_channel, cmap='gray')\naxs[0].set_title('L Channel')\naxs[1].imshow(ab_channel)\naxs[1].set_title('AB Channel')\n\n# Show the plot\nplt.show()","metadata":{"id":"eBgrZ9FBG2I4","outputId":"0c5ed2b1-c531-48d7-9f24-fe3b70b92a02","execution":{"iopub.status.busy":"2024-05-30T09:18:08.144534Z","iopub.status.idle":"2024-05-30T09:18:08.145028Z","shell.execute_reply.started":"2024-05-30T09:18:08.144784Z","shell.execute_reply":"2024-05-30T09:18:08.144804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, valid_data, test_data = split_data(\n    dataset=dataset,\n    train_size=0.6,\n    valid_size=0.2,\n    test_size=0.2\n  )","metadata":{"id":"LrHXj4KXvG1q","execution":{"iopub.status.busy":"2024-05-30T09:18:08.146723Z","iopub.status.idle":"2024-05-30T09:18:08.147054Z","shell.execute_reply.started":"2024-05-30T09:18:08.146892Z","shell.execute_reply":"2024-05-30T09:18:08.146906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data), len(valid_data), len(test_data))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:08.147923Z","iopub.status.idle":"2024-05-30T09:18:08.148220Z","shell.execute_reply.started":"2024-05-30T09:18:08.148069Z","shell.execute_reply":"2024-05-30T09:18:08.148081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.batch(32)\nvalid_data = valid_data.batch(32)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:08.149386Z","iopub.status.idle":"2024-05-30T09:18:08.149751Z","shell.execute_reply.started":"2024-05-30T09:18:08.149555Z","shell.execute_reply":"2024-05-30T09:18:08.149569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TWO BRANCH INCEPTION V3","metadata":{"id":"rw7flHvmgbsk"}},{"cell_type":"code","source":"module_filters = [\n    [64, 48, 64, 64, 96, 96, 32],\n    [64, 48, 64, 64, 96, 96, 64],\n    [64, 48, 64, 64, 96, 96, 64],\n    [192, 128, 128, 192, 128, 128, 128, 128, 192, 192],\n    [192, 160, 160, 192, 160, 160, 160, 160, 192, 192],\n    [192, 160, 160, 192, 160, 160, 160, 160, 192, 192],\n    [192, 192, 192, 192, 192, 192, 192, 192, 192, 192],\n    [192, 192, 192, 192, 192, 192, 192, 192, 192, 192],\n    [320, 384, 384, 384, 448, 384, 384, 384, 192],\n    [320, 384, 384, 384, 448, 384, 384, 384, 192],\n]","metadata":{"id":"nKwcfSi3tout","execution":{"iopub.status.busy":"2024-05-30T09:18:08.150895Z","iopub.status.idle":"2024-05-30T09:18:08.151224Z","shell.execute_reply.started":"2024-05-30T09:18:08.151063Z","shell.execute_reply":"2024-05-30T09:18:08.151078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CopyChannels(tf.keras.layers.Layer):\n    \"\"\"\n    This layer copies channels from channel_start the number of channels given in channel_count.\n    \"\"\"\n    def __init__(self,\n                 channel_start=0,\n                 channel_count=1,\n                 **kwargs):\n        self.channel_start=channel_start\n        self.channel_count=channel_count\n        super(CopyChannels, self).__init__(**kwargs)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1], self.channel_count)\n\n    def call(self, x):\n        return x[:, :, self.channel_start:(self.channel_start+self.channel_count)]\n\n    def get_config(self):\n        config = {\n            'channel_start': self.channel_start,\n            'channel_count': self.channel_count\n        }\n        base_config = super(CopyChannels, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"id":"pGNV1LfUKu19","execution":{"iopub.status.busy":"2024-05-30T09:18:08.152817Z","iopub.status.idle":"2024-05-30T09:18:08.153123Z","shell.execute_reply.started":"2024-05-30T09:18:08.152970Z","shell.execute_reply":"2024-05-30T09:18:08.152982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv2d_bn(x, filters, kernel_size, padding='same', strides=1, name=None):\n    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=False)(x)\n    x = tf.keras.layers.BatchNormalization(scale=False)(x)\n    return tf.keras.layers.Activation('relu')(x)","metadata":{"id":"3aURrrdGnJo4","execution":{"iopub.status.busy":"2024-05-30T09:18:08.154311Z","iopub.status.idle":"2024-05-30T09:18:08.154669Z","shell.execute_reply.started":"2024-05-30T09:18:08.154472Z","shell.execute_reply":"2024-05-30T09:18:08.154486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inception_module_1(x, filters):\n  t1 = conv2d_bn(x, filters[0], 1)\n\n  t2 = conv2d_bn(x, filters[1], 1)\n  t2 = conv2d_bn(t2, filters[2], 3)\n\n  t3 = conv2d_bn(x, filters[3], 1)\n  t3 = conv2d_bn(t3, filters[4], 3)\n  t3 = conv2d_bn(t3, filters[5], 3)\n\n  t4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)\n  t4 = conv2d_bn(t4, filters[6], 1)\n\n  return tf.keras.layers.concatenate([t1, t2, t3, t4], axis=3)","metadata":{"id":"WgwInXkfoL9C","execution":{"iopub.status.busy":"2024-05-30T09:18:08.155794Z","iopub.status.idle":"2024-05-30T09:18:08.156119Z","shell.execute_reply.started":"2024-05-30T09:18:08.155953Z","shell.execute_reply":"2024-05-30T09:18:08.155967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inception_module_2(x, filters):\n  t1 = conv2d_bn(x, filters[0], 1)\n\n  t2 = conv2d_bn(x, filters[1], 1)\n  t2 = conv2d_bn(t2, filters[2], (1,7))\n  t2 = conv2d_bn(t2, filters[3], (7,1))\n\n  t3 = conv2d_bn(x, filters[4], 1)\n  t3 = conv2d_bn(t3, filters[5], (1,7))\n  t3 = conv2d_bn(t3, filters[6], (7,1))\n  t3 = conv2d_bn(t3, filters[7], (1,7))\n  t3 = conv2d_bn(t3, filters[8], (7,1))\n\n  t4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)\n  t4 = conv2d_bn(t4, filters[9], 1)\n\n  return tf.keras.layers.concatenate([t1, t2, t3, t4], axis=3)","metadata":{"id":"KGPcLtFsolAe","execution":{"iopub.status.busy":"2024-05-30T09:18:08.157378Z","iopub.status.idle":"2024-05-30T09:18:08.157734Z","shell.execute_reply.started":"2024-05-30T09:18:08.157534Z","shell.execute_reply":"2024-05-30T09:18:08.157547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inception_module_3(x, filters):\n  t1 = conv2d_bn(x, filters[0], 1)\n\n  t2 = conv2d_bn(x, filters[1], 1)\n  t2_1 = conv2d_bn(t2, filters[2], (1,3))\n  t2_2 = conv2d_bn(t2, filters[3], (3,1))\n  t2 = tf.keras.layers.concatenate([t2_1, t2_2], axis=3)\n\n  t3 = conv2d_bn(x, filters[4], 1)\n  t3 = conv2d_bn(t3, filters[5], 1)\n  t3_1 = conv2d_bn(t3, filters[6], (1,3))\n  t3_2 = conv2d_bn(t3, filters[7], (3,1))\n  t3 = tf.keras.layers.concatenate([t3_1, t3_2], axis=3)\n\n  t4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)\n  t4 = conv2d_bn(t4, filters[8], 1)\n\n  return tf.keras.layers.concatenate([t1, t2, t3, t4], axis=3)","metadata":{"id":"UL28ZsICqRw7","execution":{"iopub.status.busy":"2024-05-30T09:18:08.159184Z","iopub.status.idle":"2024-05-30T09:18:08.159489Z","shell.execute_reply.started":"2024-05-30T09:18:08.159336Z","shell.execute_reply":"2024-05-30T09:18:08.159349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def two_path_inception_v3(\n                include_top=True,\n                include_first_block=True,\n                weights=None, #'two_paths_plant_leafs'\n                input_shape=(128,128,3),\n                pooling=None,\n                classes=38,\n                two_paths_partial_first_block=0,\n                two_paths_first_block=False,\n                l_ratio=0.2,\n                ab_ratio=0.8,\n                max_mix_idx=10,\n                model_name='two_path_inception_v3',\n                **kwargs):\n\n    img_input = tf.keras.layers.Input(shape=input_shape)\n\n    if include_first_block:\n        if two_paths_first_block:\n            if (l_ratio>0):\n                l_branch = CopyChannels(0,1)(img_input)\n                l_branch = conv2d_bn(l_branch, int(round(32*l_ratio)), (3, 3), strides=(2, 2), padding='valid')\n                l_branch = conv2d_bn(l_branch, int(round(32*l_ratio)), (3, 3), padding='valid')\n                l_branch = conv2d_bn(l_branch, int(round(64*l_ratio)), (3, 3))\n                l_branch = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(l_branch)\n\n            if (ab_ratio>0):\n                ab_branch = CopyChannels(1,2)(img_input)\n                ab_branch = conv2d_bn(ab_branch, int(round(32*ab_ratio)), (3, 3), strides=(2, 2), padding='valid')\n                ab_branch = conv2d_bn(ab_branch, int(round(32*ab_ratio)), (3, 3), padding='valid')\n                ab_branch = conv2d_bn(ab_branch, int(round(64*ab_ratio)), (3, 3))\n                ab_branch = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(ab_branch)\n\n            if (l_ratio>0):\n                if (ab_ratio>0):\n                    x = tf.keras.layers.Concatenate(axis=3, name='concat_first_block')([l_branch, ab_branch])\n                else:\n                    x = l_branch\n            else:\n                x = ab_branch\n        else:\n            single_branch = conv2d_bn(img_input, 32, (3, 3), strides=(2, 2), padding='valid')\n            single_branch = conv2d_bn(single_branch, 32, (3, 3), padding='valid')\n            single_branch = conv2d_bn(single_branch, 64, (3, 3))\n            single_branch = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(single_branch)\n            # print('single path first block')\n            x = single_branch\n\n    if max_mix_idx >= 0:\n        for i in range(max_mix_idx):\n          if i < 3:\n            x = inception_module_1(x, module_filters[i])\n          elif i < 8:\n            x = inception_module_2(x, module_filters[i])\n          else:\n            x = inception_module_3(x, module_filters[i])\n\n    if include_top:\n        # Classification block\n        x = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(x)\n        x = tf.keras.layers.Dense(classes, activation='softmax', name='predictions')(x)\n    else:\n        if pooling == 'avg':\n            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n\n    inputs = img_input\n    # Create model.\n    model = tf.keras.models.Model(inputs, x, name=model_name)\n    return model","metadata":{"id":"zyOBZG-RgvCQ","execution":{"iopub.status.busy":"2024-05-30T09:18:08.160918Z","iopub.status.idle":"2024-05-30T09:18:08.161276Z","shell.execute_reply.started":"2024-05-30T09:18:08.161104Z","shell.execute_reply":"2024-05-30T09:18:08.161119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPERIMENTS","metadata":{}},{"cell_type":"code","source":"print(\"Train data shape:\", tf.data.experimental.cardinality(train_data).numpy())\nprint(\"Valid data shape:\", tf.data.experimental.cardinality(valid_data).numpy())","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:08.162388Z","iopub.status.idle":"2024-05-30T09:18:08.162742Z","shell.execute_reply.started":"2024-05-30T09:18:08.162541Z","shell.execute_reply":"2024-05-30T09:18:08.162554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect the shape of one element of train_data\nfor image, label in train_data.take(5):\n    print(\"Image shape:\", image.shape)\n    print(\"Label shape:\", label.shape)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:08.164166Z","iopub.status.idle":"2024-05-30T09:18:08.164485Z","shell.execute_reply.started":"2024-05-30T09:18:08.164326Z","shell.execute_reply":"2024-05-30T09:18:08.164340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n        loss='categorical_crossentropy',\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n        metrics=['accuracy'])","metadata":{"id":"WAIewJm43Dwe","execution":{"iopub.status.busy":"2024-05-30T09:18:08.166030Z","iopub.status.idle":"2024-05-30T09:18:08.166332Z","shell.execute_reply.started":"2024-05-30T09:18:08.166181Z","shell.execute_reply":"2024-05-30T09:18:08.166193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monitor='val_accuracy'\nepochs=10\n\nhistory = model.fit(\n    train_data,\n    verbose=1,\n    epochs=epochs,\n    validation_data=valid_data\n)","metadata":{"id":"QVLSZj_53pcl","outputId":"0d106bc7-7845-4fb1-a683-9ab67cab28c0","execution":{"iopub.status.busy":"2024-05-30T09:18:08.167197Z","iopub.status.idle":"2024-05-30T09:18:08.167534Z","shell.execute_reply.started":"2024-05-30T09:18:08.167366Z","shell.execute_reply":"2024-05-30T09:18:08.167380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss_curves(history)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T09:18:08.168818Z","iopub.status.idle":"2024-05-30T09:18:08.169125Z","shell.execute_reply.started":"2024-05-30T09:18:08.168964Z","shell.execute_reply":"2024-05-30T09:18:08.168977Z"},"trusted":true},"execution_count":null,"outputs":[]}]}